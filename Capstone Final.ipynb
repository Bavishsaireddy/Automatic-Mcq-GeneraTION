{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340ab609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ce2980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import PyPDF2\n",
    "import pprint\n",
    "import itertools\n",
    "import re\n",
    "import pke\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from pywsd.similarity import max_similarity\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.lesk import cosine_lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from summarizer import Summarizer\n",
    "from flashtext.keyword import KeywordProcessor\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4229d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('popular')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa754581",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = open(r\"C:\\Users\\Hp\\Downloads\\Hyderabad.pdf\", 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "num_pages = len(pdf_reader.pages)\n",
    "\n",
    "full_text = ''\n",
    "for i in range(num_pages):\n",
    "    page = pdf_reader.pages[i]\n",
    "    full_text += page.extract_text()\n",
    "\n",
    "pdf_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0735dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyderabad, also known as the \"City of Pearls,\" is a vibrant and bustling city in southern India. With a \n",
      "population of over 10 million people, Hyderabad is the capit al city of the Indian state of Telangana. Today, the \n",
      "Charminar is one of the most visited tourist attractions in Hyderabad, and it is a symbol of the city's rich \n",
      "cultural heritage. The fort has a rich history, having been the site of numerous battles and sieges over \n",
      "the centuries. Today, visitors to the fort can explore its sprawling walls and towers, as well as its many \n",
      "underground chambers and tunn els. Hyderabad is also known for its delicious cuisine, which is a blend of Mughal, Persian, and local Telugu \n",
      "flavors. In addition to its famous landmarks and cuisine, Hyderabad is also a center of technology and \n",
      "innovation. The city has taken steps to address these challenges, including investing in public \n",
      "transportation and green spaces, and promoting sust ainable development. During the festival, the city comes alive with music, dance, and colorful \n",
      "decorations, as residents and visitors alike take part in the festivities. Another important festival in Hyderabad is the Bonalu, a traditional Telangana festival that celebrates th e \n",
      "goddess Mahankali. In conclusion, Hyderabad is a city that combines rich history, vibrant culture, and technological \n",
      "innovation.\n"
     ]
    }
   ],
   "source": [
    "model = Summarizer()\n",
    "result = model(full_text, min_length=60, max_length = 500 , ratio = 0.4)\n",
    "summarized_text = ''.join(result)\n",
    "print (summarized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2931f7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hyderabad', 'mughals', 'charminar', 'telangana', 'persian', 'hindu', 'ganesha', 'nizams', 'ganesh chaturthi', 'city', 'pearls', '-day', 'bonalu', 'telugu', 'british', 'golconda fort', 'india', 'biryani', 'tunn els', 'orig']\n",
      "['hyderabad', 'charminar', 'telangana', 'persian', 'city', 'pearls', 'bonalu', 'telugu', 'india', 'tunn els']\n"
     ]
    }
   ],
   "source": [
    "def get_nouns_multipartite(text):\n",
    "    out=[]\n",
    "\n",
    "    extractor = pke.unsupervised.MultipartiteRank()\n",
    "    extractor.load_document(input=text)\n",
    "    #    not contain punctuation marks or stopwords as candidates.\n",
    "    pos = {'PROPN'}\n",
    "    #pos = {'VERB', 'ADJ', 'NOUN'}\n",
    "    stoplist = list(string.punctuation)\n",
    "    stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "    stoplist += stopwords.words('english')\n",
    "    extractor.candidate_selection(pos=pos)\n",
    "    # 4. build the Multipartite graph and rank candidates using random walk,\n",
    "    #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
    "    #    threshold/method parameters.\n",
    "    extractor.candidate_weighting(alpha=1.1,threshold=0.75,method='average')\n",
    "    keyphrases = extractor.get_n_best(n=20)\n",
    "\n",
    "    for key in keyphrases:\n",
    "        out.append(key[0])\n",
    "\n",
    "    return out\n",
    "\n",
    "keywords = get_nouns_multipartite(full_text) \n",
    "print (keywords)\n",
    "filtered_keys=[]\n",
    "for keyword in keywords:\n",
    "    if keyword.lower() in summarized_text.lower():\n",
    "        filtered_keys.append(keyword)\n",
    "        \n",
    "print (filtered_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3fe89e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    sentences = [sent_tokenize(text)]\n",
    "    sentences = [y for x in sentences for y in x]\n",
    "    # Remove any short sentences less than 20 letters.\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e910d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_for_keyword(keywords, sentences):\n",
    "    keyword_processor = KeywordProcessor()\n",
    "    keyword_sentences = {}\n",
    "    for word in keywords:\n",
    "        keyword_sentences[word] = []\n",
    "        keyword_processor.add_keyword(word)\n",
    "    for sentence in sentences:\n",
    "        keywords_found = keyword_processor.extract_keywords(sentence)\n",
    "        for key in keywords_found:\n",
    "            keyword_sentences[key].append(sentence)\n",
    "\n",
    "    for key in keyword_sentences.keys():\n",
    "        values = keyword_sentences[key]\n",
    "        values = sorted(values, key=len, reverse=True)\n",
    "        keyword_sentences[key] = values\n",
    "    return keyword_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05122f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hyderabad': [\"Today, the \\nCharminar is one of the most visited tourist attractions in Hyderabad, and it is a symbol of the city's rich \\ncultural heritage.\", 'Another important festival in Hyderabad is the Bonalu, a traditional Telangana festival that celebrates th e \\ngoddess Mahankali.', 'Hyderabad is also known for its delicious cuisine, which is a blend of Mughal, Persian, and local Telugu \\nflavors.', 'With a \\npopulation of over 10 million people, Hyderabad is the capit al city of the Indian state of Telangana.', 'In conclusion, Hyderabad is a city that combines rich history, vibrant culture, and technological \\ninnovation.', 'In addition to its famous landmarks and cuisine, Hyderabad is also a center of technology and \\ninnovation.', 'Hyderabad, also known as the \"City of Pearls,\" is a vibrant and bustling city in southern India.'], 'charminar': [\"Today, the \\nCharminar is one of the most visited tourist attractions in Hyderabad, and it is a symbol of the city's rich \\ncultural heritage.\"], 'telangana': ['Another important festival in Hyderabad is the Bonalu, a traditional Telangana festival that celebrates th e \\ngoddess Mahankali.', 'With a \\npopulation of over 10 million people, Hyderabad is the capit al city of the Indian state of Telangana.'], 'persian': ['Hyderabad is also known for its delicious cuisine, which is a blend of Mughal, Persian, and local Telugu \\nflavors.'], 'city': ['The city has taken steps to address these challenges, including investing in public \\ntransportation and green spaces, and promoting sust ainable development.', 'During the festival, the city comes alive with music, dance, and colorful \\ndecorations, as residents and visitors alike take part in the festivities.', \"Today, the \\nCharminar is one of the most visited tourist attractions in Hyderabad, and it is a symbol of the city's rich \\ncultural heritage.\", 'With a \\npopulation of over 10 million people, Hyderabad is the capit al city of the Indian state of Telangana.', 'In conclusion, Hyderabad is a city that combines rich history, vibrant culture, and technological \\ninnovation.', 'Hyderabad, also known as the \"City of Pearls,\" is a vibrant and bustling city in southern India.', 'Hyderabad, also known as the \"City of Pearls,\" is a vibrant and bustling city in southern India.'], 'pearls': ['Hyderabad, also known as the \"City of Pearls,\" is a vibrant and bustling city in southern India.'], 'bonalu': ['Another important festival in Hyderabad is the Bonalu, a traditional Telangana festival that celebrates th e \\ngoddess Mahankali.'], 'telugu': ['Hyderabad is also known for its delicious cuisine, which is a blend of Mughal, Persian, and local Telugu \\nflavors.'], 'india': ['Hyderabad, also known as the \"City of Pearls,\" is a vibrant and bustling city in southern India.'], 'tunn els': ['Today, visitors to the fort can explore its sprawling walls and towers, as well as its many \\nunderground chambers and tunn els.']}\n"
     ]
    }
   ],
   "source": [
    "sentences = tokenize_sentences(summarized_text)\n",
    "keyword_sentence_mapping = get_sentences_for_keyword(filtered_keys, sentences)       \n",
    "print (keyword_sentence_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2056ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distractors_wordnet(syn,word):\n",
    "    distractors=[]\n",
    "    word= word.lower()\n",
    "    orig_word = word\n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    hypernym = syn.hypernyms()\n",
    "    if len(hypernym) == 0: \n",
    "        return distractors\n",
    "    for item in hypernym[0].hyponyms():\n",
    "        name = item.lemmas()[0].name()\n",
    "        #print (\"name \",name, \" word\",orig_word)\n",
    "        if name == orig_word:\n",
    "            continue\n",
    "        name = name.replace(\"_\",\" \")\n",
    "        name = \" \".join(w.capitalize() for w in name.split())\n",
    "        if name is not None and name not in distractors:\n",
    "            distractors.append(name)\n",
    "    return distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef471847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordsense(sent,word):\n",
    "    word= word.lower()\n",
    "    \n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    \n",
    "    \n",
    "    synsets = wn.synsets(word,'n')\n",
    "    if synsets:\n",
    "        wup = max_similarity(sent, word, 'wup', pos='n')\n",
    "        adapted_lesk_output =  adapted_lesk(sent, word, pos='n')\n",
    "        lowest_index = min (synsets.index(wup),synsets.index(adapted_lesk_output))\n",
    "        return synsets[lowest_index]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac4cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distractors from http://conceptnet.io/\n",
    "def get_distractors_conceptnet(word):\n",
    "    word = word.lower()\n",
    "    original_word= word\n",
    "    if (len(word.split())>0):\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    distractor_list = [] \n",
    "    url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n",
    "    obj = requests.get(url).json()\n",
    "\n",
    "    for edge in obj['edges']:\n",
    "        link = edge['end']['term'] \n",
    "\n",
    "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "        obj2 = requests.get(url2).json()\n",
    "        for edge in obj2['edges']:\n",
    "            word2 = edge['start']['label']\n",
    "            if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
    "                distractor_list.append(word2)\n",
    "                   \n",
    "    return distractor_list\n",
    "\n",
    "key_distractor_list = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "840f672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword in keyword_sentence_mapping:\n",
    "    if len(keyword_sentence_mapping[keyword]) > 0:\n",
    "        wordsense = get_wordsense(keyword_sentence_mapping[keyword][0],keyword)\n",
    "        if wordsense:\n",
    "            distractors = get_distractors_wordnet(wordsense,keyword)\n",
    "            if len(distractors) == 0:\n",
    "                distractors = get_distractors_conceptnet(keyword)\n",
    "            if len(distractors) != 0:\n",
    "                key_distractor_list[keyword] = distractors\n",
    "        else:\n",
    "            distractors = get_distractors_conceptnet(keyword)\n",
    "            if len(distractors) != 0:\n",
    "                key_distractor_list[keyword] = distractors\n",
    "\n",
    "    index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca893414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Today, the \n",
      "Charminar is one of the most visited tourist attractions in  _______ , and it is a symbol of the city's rich \n",
      "cultural heritage.\n",
      "\t a )   Hyderabad\n",
      "\t b )   Tien Shan\n",
      "\t c )   Brahmaputra\n",
      "\t d )   Bangalore\n",
      "\n",
      "More options:  ['Mumbai', 'Nanda Devi', 'Gujarat', 'Sikkim', 'Tamil Nadu', 'Punjab', 'Assam', 'Tirich Mir', 'Islamabad', 'Rawalpindi', 'Sind', 'Peshawar', 'Great Indian Desert', 'Khyber Pass', 'Kashmir', 'Pamir Mountains'] \n",
      "\n",
      "\n",
      "2) Hyderabad is also known for its delicious cuisine, which is a blend of Mughal,  _______ , and local Telugu \n",
      "flavors.\n",
      "\t a )   Afghan\n",
      "\t b )   Altaic\n",
      "\t c )   Persian\n",
      "\t d )   Armenian\n",
      "\n",
      "More options:  ['Bangladeshi', 'Bengali', 'Bhutanese', 'Burmese', 'Byzantine', 'Cambodian', 'Chinese', 'Coolie', 'East Indian', 'Eurasian', 'Hindu', 'Hmong', 'Iberian', 'Indian', 'Indonesian', 'Irani'] \n",
      "\n",
      "\n",
      "3) The  _______  has taken steps to address these challenges, including investing in public \n",
      "transportation and green spaces, and promoting sust ainable development.\n",
      "\t a )   City\n",
      "\t b )   Town\n",
      "\n",
      "More options:  [] \n",
      "\n",
      "\n",
      "4) Hyderabad, also known as the \"City of  _______ ,\" is a vibrant and bustling city in southern India.\n",
      "\t a )   Diamond\n",
      "\t b )   Emerald\n",
      "\t c )   Pearls\n",
      "\t d )   Crown Jewel\n",
      "\n",
      "More options:  ['Pearl', 'Ruby', 'Sapphire', 'Solitaire'] \n",
      "\n",
      "\n",
      "5) Hyderabad is also known for its delicious cuisine, which is a blend of Mughal, Persian, and local  _______  \n",
      "flavors.\n",
      "\t a )   Gond\n",
      "\t b )   Telugu\n",
      "\t c )   Badaga\n",
      "\t d )   Gadaba\n",
      "\n",
      "More options:  ['Kanarese', 'Kolam', 'Kota', 'Kui', 'Malto', 'Savara', 'Tamil', 'Telugu', 'Toda', 'Tulu'] \n",
      "\n",
      "\n",
      "6) Hyderabad, also known as the \"City of Pearls,\" is a vibrant and bustling city in southern  _______ .\n",
      "\t a )   Tartary\n",
      "\t b )   India\n",
      "\t c )   Tajikistan\n",
      "\t d )   Lebanon\n",
      "\n",
      "More options:  ['Asian country', 'Roman Empire', 'Qatar', 'Turkistan', 'Tibet', 'Kuwait', 'South Korea'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each in key_distractor_list:\n",
    "    sentence = keyword_sentence_mapping[each][0]\n",
    "    pattern = re.compile(each, re.IGNORECASE)\n",
    "    output = pattern.sub( \" _______ \", sentence)\n",
    "    print (\"%s)\"%(index),output)\n",
    "    choices = [each.capitalize()] + key_distractor_list[each]\n",
    "    top4choices = choices[:4]\n",
    "    random.shuffle(top4choices)\n",
    "    optionchoices = ['a','b','c','d']\n",
    "    for idx,choice in enumerate(top4choices):\n",
    "        print (\"\\t\",optionchoices[idx],\")\",\" \",choice)\n",
    "    print (\"\\nMore options: \", choices[4:20],\"\\n\\n\")\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8f081cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Define the main function that generates the quiz\n",
    "def generate_quiz():\n",
    "    global sentence_labels, choice_labels\n",
    "    \n",
    "    # Clear any existing quiz questions\n",
    "    for label in sentence_labels:\n",
    "        label.destroy()\n",
    "    for label in choice_labels:\n",
    "        label.destroy()\n",
    "    \n",
    "    # Generate a new quiz\n",
    "    index = 1\n",
    "    for each in key_distractor_list:\n",
    "        sentence = keyword_sentence_mapping[each][0]\n",
    "        pattern = re.compile(each, re.IGNORECASE)\n",
    "        output = pattern.sub( \" _______ \", sentence)\n",
    "        \n",
    "        # Create a label for the sentence and add it to the GUI\n",
    "        sentence_label = tk.Label(root, text=\"{}. {}\".format(index, output))\n",
    "        sentence_label.pack()\n",
    "        sentence_labels.append(sentence_label)\n",
    "        \n",
    "        choices = [each.capitalize()] + key_distractor_list[each]\n",
    "        top4choices = choices[:4]\n",
    "        random.shuffle(top4choices)\n",
    "        \n",
    "        # Create labels for the answer choices and add them to the GUI\n",
    "        for idx,choice in enumerate(top4choices):\n",
    "            choice_label = tk.Label(root, text=\"{}. {}\".format(optionchoices[idx], choice))\n",
    "            choice_label.pack()\n",
    "            choice_labels.append(choice_label)\n",
    "        \n",
    "        index = index + 1\n",
    "    \n",
    "    # Add a button to generate a new quiz\n",
    "    generate_button = tk.Button(root, text=\"Generate Quiz\", command=generate_quiz)\n",
    "    generate_button.pack()\n",
    "\n",
    "# Define the main window and initialize some variables\n",
    "root = tk.Tk()\n",
    "root.title(\"Keyword Quiz\")\n",
    "sentence_labels = []\n",
    "choice_labels = []\n",
    "optionchoices = ['i','ii','iii','iv']\n",
    "\n",
    "# Call the generate_quiz function to create the initial quiz\n",
    "generate_quiz()\n",
    "\n",
    "# Start the main event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e174e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9739e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5191ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42544d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
